# TETRIS_DQN ğŸ®ğŸ§ 

A Deep Q-Network (DQN) agent trained to play Tetris using PyTorch and Pygame. The project integrates a fully functional game engine, custom graphics, and a reinforcement learning loop capable of learning from scratch. Users can play manually or watch the agent play.

![](assets/images/screen.png)

---

## ğŸ“ Project Structure
```text
TETRIS_DQN/
â”œâ”€â”€ assets/                # Fonts and sound effects
â”‚    â”œâ”€â”€ audio/            # Game sound effects (drop, clear, game over, etc.)
â”‚    â””â”€â”€ fonts/            # Font files for rendering UI
â”œâ”€â”€ chkpts/                # Checkpointed models (.pth)
â”œâ”€â”€ game/                  # Game engine and graphics
â”‚    â”œâ”€â”€ Graphics.py       # Handles rendering, font loading, colors
â”‚    â”œâ”€â”€ Piece.py          # Piece shapes, rotations, and color mapping
â”‚    â””â”€â”€ TetrisEngine.py   # GameState class, physics, scoring
â”œâ”€â”€ logs/                  # PNG plot with reward/loss/q_diff training curves
â”œâ”€â”€ models/                # Trained models (final .pth weights)
â”œâ”€â”€ rl/                    # Reinforcement Learning components
â”‚    â”œâ”€â”€ dqn_agent.py      # DQNAgent with replay buffer, epsilon-greedy logic
â”‚    â””â”€â”€ tetris_env.py     # Gym-style wrapper around GameState
â”œâ”€â”€ scripts/               # Training and evaluation
â”‚    â”œâ”€â”€ train.py          # Runs DQN training loop and saves model
â”‚    â””â”€â”€ test.py           # Contains auto_play logic for inference
â””â”€â”€ main.py                # GUI launcher with menu: manual play or AI autoplay
```

---

## ğŸ§  Reinforcement Learning Overview

The project uses a Deep Q-Learning (DQN) approach with experience replay and target network updates.

### ğŸ” Reward Function
- **Lines cleared**: Reward = `clear_countÂ² * factor`
- **Height penalty**: Encourages flatter surfaces
- **Holes penalty**: Penalizes holes created
- **Game Over**: Heavy penalty `-5`

### ğŸ“Š Logging
Training logs (reward, loss, and Q-value difference) are visualized in a single `training_plot_ep{episode}.png` located in `/logs`.

---

## ğŸš€ How to Run

### âœ… Prerequisites
Install dependencies:
```bash
pip install -r requirements.txt
```

### ğŸ® Launch Game
Run the main launcher, which lets you select manual play or AI autoplay:
```bash
python main.py
```
- Manual Play: Control the pieces yourself using the keyboard.
- Autoplay: Watch the trained DQN agent play automatically.

Note: The scripts/test.py file contains the autoplay logic but is not intended to be run standalone. It is called by main.py when the user chooses AI mode.

### ğŸ§ª Train the Agent
Start training the DQN agent:
```bash
python scripts/train.py
```
- Checkpoints are saved periodically to the chkpts/ directory.
- The final trained model is saved in the models/ directory.
- Training metrics (reward, loss, Q-diff) are saved as a single combined plot image in logs/training_plot.png.

![](assets/images/game.png)

---

## ğŸ“Š Logs & Visualization
- Training progress is saved as a single image training_plot.png inside the logs/ folder.
- This PNG shows the evolution of:
    - Total episode rewards
    - Training loss
    - Average Q-value differences (q_diff)
    
No TensorBoard is used; all plots are pre-generated and saved as PNG for convenience.

---

## ğŸ” Additional Notes
- The audio effects and fonts are loaded from the assets/audio/ and assets/fonts/ folders respectively.
- The model files are saved as PyTorch .pth files.
- The game engine handles piece rendering, collision detection, line clearing, and scoring.
- The DQN uses experience replay and epsilon-greedy exploration with decay.

![](assets\images\end.png)

